{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A Tale of Two cities\n",
    "Clustering the Neighbourhoods of London and Paris\n",
    "Vridhi Patel\n",
    "\n",
    "\n",
    "\n",
    "- Introduction\n",
    "    A Tale of Two cities, a novel written by Charles Dickens was set in London and Paris which takes place during the French Revolution. These cities were both happening then and now. A lot has changed over the years and we now take a look at how the cities have grown.\n",
    "\n",
    "    London and Paris are quite the popular tourist and vacation destinations for people all around the world. They are diverse and multicultural and offer a wide variety of experiences that is widely sought after. We try to group the neighbourhoods of London and Paris respectively and draw insights to what they look like now.\n",
    "\n",
    "- Business Problem\n",
    "    The aim is to help tourists choose their destinations depending on the experiences that the neighbourhoods have to offer and what they would want to have. This also helps people make decisions if they are thinking about migrating to London or Paris or even if they want to relocate neighbourhoods within the city. Our findings will help stakeholders make informed decisions and address any concerns they have including the different kinds of cuisines, provision stores and what the city has to offer.\n",
    "\n",
    "- Data Description\n",
    "    We require geolocation data for both London and Paris. Postal codes in each city serve as a starting point. Using Postal codes we use can find out the neighbourhoods, boroughs, venues and their most popular venue categories.\n",
    "\n",
    "    London\n",
    "    To derive our solution, We scrape our data from https://en.wikipedia.org/wiki/List_of_areas_of_London\n",
    "\n",
    "    This wikipedia page has information about all the neighbourhoods, we limit it London.\n",
    "\n",
    "    borough : Name of Neighbourhood\n",
    "    town : Name of borough\n",
    "    post_code : Postal codes for London.\n",
    "    This wikipedia page lacks information about the geographical locations. To solve this problem we use ArcGIS API\n",
    "\n",
    "- ArcGIS API\n",
    "    ArcGIS Online enables you to connect people, locations, and data using interactive maps. Work with smart, data-driven styles and intuitive analysis tools that deliver location intelligence. Share your insights with the world or specific groups.\n",
    "\n",
    "    More specifically, we use ArcGIS to get the geo locations of the neighbourhoods of London. The following columns are added to our initial dataset which prepares our data.\n",
    "\n",
    "    latitude : Latitude for Neighbourhood\n",
    "    longitude : Longitude for Neighbourhood\n",
    "    Paris\n",
    "    To derive our solution, We leverage JSON data available at https://www.data.gouv.fr/fr/datasets/r/e88c6fda-1d09-42a0-a069-606d3259114e\n",
    "\n",
    "    The JSON file has data about all the neighbourhoods in France, we limit it to Paris.\n",
    "\n",
    "    postal_code : Postal codes for France\n",
    "    nom_comm : Name of Neighbourhoods in France\n",
    "    nom_dept : Name of the boroughs, equivalent to towns in France\n",
    "    geo_point_2d : Tuple containing the latitude and longitude of the Neighbourhoods.\n",
    "    Foursquare API Data\n",
    "    We will need data about different venues in different neighbourhoods of that specific borough. In order to gain that information we will use \"Foursquare\" locational information. Foursquare is a location data provider with information about all manner of venues and events within an area of interest. Such information includes venue names, locations, menus and even photos. As such, the foursquare location platform will be used as the sole data source since all the stated required information can be obtained through the API.\n",
    "\n",
    "    After finding the list of neighbourhoods, we then connect to the Foursquare API to gather information about venues inside each and every neighbourhood. For each neighbourhood, we have chosen the radius to be 500 meters.\n",
    "\n",
    "    The data retrieved from Foursquare contained information of venues within a specified distance of the longitude and latitude of the postcodes. The information obtained per venue as follows:\n",
    "\n",
    "    Neighbourhood : Name of the Neighbourhood\n",
    "    Neighbourhood Latitude : Latitude of the Neighbourhood\n",
    "    Neighbourhood Longitude : Longitude of the Neighbourhood\n",
    "    Venue : Name of the Venue\n",
    "    Venue Latitude : Latitude of Venue\n",
    "    Venue Longitude : Longitude of Venue\n",
    "    Venue Category : Category of Venue\n",
    "    Based on all the information collected for both London and Paris, we have sufficient data to build our model. We cluster the neighbourhoods together based on similar venue categories. We then present our observations and findings. Using this data, our stakeholders can take the necessary decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting folium\n",
      "  Downloading folium-0.12.1-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.9 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from folium) (3.0.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from folium) (2.25.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from folium) (1.19.2)\n",
      "Collecting branca>=0.3.0\n",
      "  Downloading branca-0.4.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from jinja2>=2.9->folium) (2.0.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests->folium) (2.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests->folium) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests->folium) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.8-main/lib/python3.8/site-packages (from requests->folium) (2021.10.8)\n",
      "Installing collected packages: branca, folium\n",
      "Successfully installed branca-0.4.2 folium-0.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install folium\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import folium\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>London borough</th>\n",
       "      <th>Post town</th>\n",
       "      <th>Postcode district</th>\n",
       "      <th>Dial code</th>\n",
       "      <th>OS grid ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbey Wood</td>\n",
       "      <td>Bexley, Greenwich [7]</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>SE2</td>\n",
       "      <td>020</td>\n",
       "      <td>TQ465785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acton</td>\n",
       "      <td>Ealing, Hammersmith and Fulham[8]</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>W3, W4</td>\n",
       "      <td>020</td>\n",
       "      <td>TQ205805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Addington</td>\n",
       "      <td>Croydon[8]</td>\n",
       "      <td>CROYDON</td>\n",
       "      <td>CR0</td>\n",
       "      <td>020</td>\n",
       "      <td>TQ375645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Addiscombe</td>\n",
       "      <td>Croydon[8]</td>\n",
       "      <td>CROYDON</td>\n",
       "      <td>CR0</td>\n",
       "      <td>020</td>\n",
       "      <td>TQ345665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albany Park</td>\n",
       "      <td>Bexley</td>\n",
       "      <td>BEXLEY, SIDCUP</td>\n",
       "      <td>DA5, DA14</td>\n",
       "      <td>020</td>\n",
       "      <td>TQ478728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>Woolwich</td>\n",
       "      <td>Greenwich</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>SE18</td>\n",
       "      <td>020</td>\n",
       "      <td>TQ435795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>Worcester Park</td>\n",
       "      <td>Sutton, Kingston upon Thames</td>\n",
       "      <td>WORCESTER PARK</td>\n",
       "      <td>KT4</td>\n",
       "      <td>020</td>\n",
       "      <td>TQ225655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>Wormwood Scrubs</td>\n",
       "      <td>Hammersmith and Fulham</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>W12</td>\n",
       "      <td>020</td>\n",
       "      <td>TQ225815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>Yeading</td>\n",
       "      <td>Hillingdon</td>\n",
       "      <td>HAYES</td>\n",
       "      <td>UB4</td>\n",
       "      <td>020</td>\n",
       "      <td>TQ115825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>Yiewsley</td>\n",
       "      <td>Hillingdon</td>\n",
       "      <td>WEST DRAYTON</td>\n",
       "      <td>UB7</td>\n",
       "      <td>020</td>\n",
       "      <td>TQ063804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>532 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Location                     London borough       Post town  \\\n",
       "0         Abbey Wood              Bexley, Greenwich [7]          LONDON   \n",
       "1              Acton  Ealing, Hammersmith and Fulham[8]          LONDON   \n",
       "2          Addington                         Croydon[8]         CROYDON   \n",
       "3         Addiscombe                         Croydon[8]         CROYDON   \n",
       "4        Albany Park                             Bexley  BEXLEY, SIDCUP   \n",
       "..               ...                                ...             ...   \n",
       "527         Woolwich                          Greenwich          LONDON   \n",
       "528   Worcester Park       Sutton, Kingston upon Thames  WORCESTER PARK   \n",
       "529  Wormwood Scrubs             Hammersmith and Fulham          LONDON   \n",
       "530          Yeading                         Hillingdon           HAYES   \n",
       "531         Yiewsley                         Hillingdon    WEST DRAYTON   \n",
       "\n",
       "    Postcode district Dial code OS grid ref  \n",
       "0                 SE2       020    TQ465785  \n",
       "1              W3, W4       020    TQ205805  \n",
       "2                 CR0       020    TQ375645  \n",
       "3                 CR0       020    TQ345665  \n",
       "4           DA5, DA14       020    TQ478728  \n",
       "..                ...       ...         ...  \n",
       "527              SE18       020    TQ435795  \n",
       "528               KT4       020    TQ225655  \n",
       "529               W12       020    TQ225815  \n",
       "530               UB4       020    TQ115825  \n",
       "531               UB7       020    TQ063804  \n",
       "\n",
       "[532 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data collection\n",
    "url_london = \"https://en.wikipedia.org/wiki/List_of_areas_of_London\"\n",
    "wiki_london_url = requests.get(url_london)\n",
    "wiki_london_data = pd.read_html(wiki_london_url.text)\n",
    "wiki_london_data = wiki_london_data[1]\n",
    "wiki_london_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O 'france-data.json' https://www.data.gouv.fr/fr/datasets/r/e88c6fda-1d09-42a0-a069-606d3259114e\n",
    "print(\"Data Downloaded!\")\n",
    "paris_raw = pd.read_json('france-data.json')\n",
    "paris_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_london_data.rename(columns=lambda x: x.strip().replace(\" \", \"_\"), inplace=True)\n",
    "wiki_london_data['borough'] = wiki_london_data['borough'].map(lambda x: x.rstrip(']').rstrip('0123456789').rstrip('['))\n",
    "For Paris, we break down each of the nested fields and create the dataframe that we need:\n",
    "\n",
    "paris_field_data = pd.DataFrame()\n",
    "for f in paris_raw.fields:\n",
    "    dict_new = f\n",
    "    paris_field_data = paris_field_data.append(dict_new, ignore_index=True)\n",
    "\n",
    "paris_field_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = wiki_london_data.drop( [ wiki_london_data.columns[0], wiki_london_data.columns[4], wiki_london_data.columns[5] ], axis=1)\n",
    "\n",
    "df_2 = paris_field_data[['postal_code','nom_comm','nom_dept','geo_point_2d']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[df1['town'].str.contains('LONDON')]\n",
    "\n",
    "df_paris = df_2[df_2['nom_dept'].str.contains('PARIS')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.geocoding import geocode\n",
    "from arcgis.gis import GIS\n",
    "gis = GIS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y_uk(address1):\n",
    "   lat_coords = 0\n",
    "   lng_coords = 0\n",
    "   g = geocode(address='{}, London, England, GBR'.format(address1))[0]\n",
    "   lng_coords = g['location']['x']\n",
    "   lat_coords = g['location']['y']\n",
    "   return str(lat_coords) +\",\"+ str(lng_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_latlng_uk = geo_coordinates_uk.apply(lambda x: get_x_y_uk(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_merged = pd.concat([df1,lat_uk.astype(float), lng_uk.astype(float)], axis=1)\n",
    "london_merged.columns= ['borough','town','post_code','latitude','longitude']\n",
    "london_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_lat = paris_latlng.apply(lambda x: x.split(',')[0])\n",
    "paris_lat = paris_lat.apply(lambda x: x.lstrip('['))\n",
    "\n",
    "paris_lng = paris_latlng.apply(lambda x: x.split(',')[1])\n",
    "paris_lng = paris_lng.apply(lambda x: x.rstrip(']'))\n",
    "\n",
    "paris_geo_lat  = pd.DataFrame(paris_lat.astype(float))\n",
    "paris_geo_lat.columns=['Latitude']\n",
    "\n",
    "paris_geo_lng = pd.DataFrame(paris_lng.astype(float))\n",
    "paris_geo_lng.columns=['Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris_combined_data = pd.concat([df_paris.drop('geo_point_2d', axis=1), paris_geo_lat, paris_geo_lng], axis=1)\n",
    "paris_combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT=100\n",
    "\n",
    "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "\n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "\n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius,\n",
    "            LIMIT\n",
    "            )\n",
    "\n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "\n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighbourhood', \n",
    "                  'Neighbourhood Latitude', \n",
    "                  'Neighbourhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Category']\n",
    "\n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "London_venue_cat = pd.get_dummies(venues_in_London[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# Adding neighbourhood to the mix\n",
    "London_venue_cat['Neighbourhood'] = venues_in_London['Neighbourhood'] \n",
    "\n",
    "# moving neighborhood column to the first column\n",
    "fixed_columns = [London_venue_cat.columns[-1]] + list(London_venue_cat.columns[:-1])\n",
    "London_venue_cat = London_venue_cat[fixed_columns]\n",
    "\n",
    "# Grouping and calculating the mean\n",
    "London_grouped = London_venue_cat.groupby('Neighbourhood').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "\n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Neighbourhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe for London\n",
    "neighborhoods_venues_sorted_london = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted_london['Neighbourhood'] = London_grouped['Neighbourhood']\n",
    "\n",
    "for ind in np.arange(London_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted_london.iloc[ind, 1:] = return_most_common_venues(London_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted_london.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "k_num_clusters = 5\n",
    "\n",
    "London_grouped_clustering = London_grouped.drop('Neighbourhood', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans_london = KMeans(n_clusters=k_num_clusters, random_state=0).fit(London_grouped_clustering)\n",
    "Our model has labelled each of the neighbourhoods, we add the label into our dataset.\n",
    "\n",
    "neighborhoods_venues_sorted_london.insert(0, 'Cluster Labels', kmeans_london.labels_ +1)\n",
    "We then join London_merged with our neighbourhood venues sorted to add latitude & longitude for each of the neighborhood to prepare it for visualization.\n",
    "\n",
    "london_data = london_merged\n",
    "\n",
    "london_data = london_data.join(neighborhoods_venues_sorted_london.set_index('Neighbourhood'), on='borough')\n",
    "\n",
    "london_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "london_data_nonan = london_data.dropna(subset=['Cluster Labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We could examine our clusters by expanding on our code using the Cluster Labels column:\n",
    "\n",
    "Cluster 1\n",
    "\n",
    "london_data_nonan.loc[london_data_nonan['Cluster Labels'] == 1, london_data_nonan.columns[[1] + list(range(5, london_data_nonan.shape[1]))]]\n",
    "Cluster 2\n",
    "\n",
    "london_data_nonan.loc[london_data_nonan['Cluster Labels'] == 2, london_data_nonan.columns[[1] + list(range(5, london_data_nonan.shape[1]))]]\n",
    "Cluster 3\n",
    "\n",
    "london_data_nonan.loc[london_data_nonan['Cluster Labels'] == 3, london_data_nonan.columns[[1] + list(range(5, london_data_nonan.shape[1]))]]\n",
    "Cluster 4\n",
    "\n",
    "london_data_nonan.loc[london_data_nonan['Cluster Labels'] == 4, london_data_nonan.columns[[1] + list(range(5, london_data_nonan.shape[1]))]]\n",
    "Cluster 5\n",
    "\n",
    "london_data_nonan.loc[london_data_nonan['Cluster Labels'] == 5, london_data_nonan.columns[[1] + list(range(5, london_data_nonan.shape[1]))]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results and Discussion The neighbourhoods of London are very mulitcultural. There are a lot of different cusines including Indian, Italian, Turkish and Chinese. London seems to take a step further in this direction by having a lot of Restaurants, bars, juice bars, coffee shops, Fish and Chips shop and Breakfast spots. It has a lot of shopping options too with that of the Flea markets, flower shops, fish markets, Fishing stores, clothing stores. The main modes of transport seem to be Buses and trains. For leisure, the neighbourhoods are set up to have lots of parks, golf courses, zoo, gyms and Historic sites. Overall, the city of London offers a multicultural, diverse and certainly an entertaining experience.\n",
    "Paris is relatively small in size geographically. It has a wide variety of cusines and eateries including French, Thai, Cambodian, Asian, Chinese etc. There are a lot of hangout spots including many Restaurants and Bars. Paris has a lot of Bistro's. Different means of public transport in Paris which includes buses, bikes, boats or ferries. For leisure and sight seeing, there are a lot of Plazas, Trails, Parks, Historic sites, clothing shops, Art galleries and Museums. Overall, Paris seems like the relaxing vacation spot with a mix of lakes, historic spots and a wide variety of cusines to try out.\n",
    "\n",
    "Conclusion The purpose of this project was to explore the cities of London and Paris and see how attractive it is to potential tourists and migrants. We explored both the cities based on their postal codes and then extrapolated the common venues present in each of the neighbourhoods finally concluding with clustering similar neighbourhoods together.\n",
    "We could see that each of the neighbourhoods in both the cities have a wide variety of experiences to offer which is unique in it's own way. The cultural diversity is quite evident which also gives the feeling of a sense of inclusion.\n",
    "\n",
    "Both Paris and London seem to offer a vacation stay or a romantic getaway with a lot of places to explore, beautiful landscapes, amazing food and a wide variety of culture. Overall, it's upto the stakeholders to decide which experience they would prefer more and which would more to their liking.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
